{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"cs109a_hw6.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# pandas tricks for better display\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "- **THIS IS AN INDIVIDUAL ASSIGNMENT. Collaboration on this homework IS NOT PERMITTED.**\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Plots should be legible and interpretable without having to refer to the code that generated them, including labels for the $x$- and $y$-axes as well as a descriptive title and/or legend when appropriate.\n",
    "- When asked to interpret a visualization, do not simply describe it (e.g., \"the curve has a steep slope up\"), but instead explain what you think the plot *means*.\n",
    "- The use of 'hard-coded' values to try and pass tests rather than solving problems programmatically will not receive credit.\n",
    "- The use of *extremely* inefficient or error-prone code (e.g., copy-pasting nearly identical commands rather than looping) may result in only partial credit.\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. Please get course staff approval before importing any additional 3rd party libraries.\n",
    "- Enable scrolling output on cells with very long output.\n",
    "- Feel free to add additional code or markdown cells as needed.\n",
    "- Ensure your code runs top to bottom without error and passes all tests by restarting the kernel and running all cells. This is how the notebook will be evaluated (note that this can take a few minutes). \n",
    "- **You should do a \"Restart Kernel and Run All Cells\" before submitting to ensure (1) your notebook actually runs and (2) all output is visible**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook contents\n",
    "\n",
    "- [**Overview and data description**](#intro)\n",
    "\n",
    "\n",
    "- [**Question 1: Decision Tree [21 pts]**](#part1)\n",
    "\n",
    "\n",
    "- [**Question 2: Bagging [20 pts]**](#part2) \n",
    "\n",
    "\n",
    "- [**Question 3: Random Forests [14 pts]**](#part3) \n",
    "\n",
    "\n",
    "- [**Question 4: Boosting [30 pts]**](#part4) \n",
    "\n",
    "\n",
    "- [**Question 5: Understanding [15 pts]**](#part5) \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"intro\"></a>\n",
    "\n",
    "## Overview and data description\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "### Higgs boson discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between collisions that produce Higgs bosons and collisions that produce only background noise. \n",
    "\n",
    "### Data description\n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle collision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background).\n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: [Baldi et al., Nature Communications 5, 2014](https://www.nature.com/articles/ncomms5308).\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "Run the following cell to load the data. Do not modify this code. We need to ensure everyone has the exact same arrays for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "# DO NOT MODIFY THIS CODE\n",
    "\n",
    "data_train = pd.read_csv(\"data/Higgs_train.csv\")\n",
    "data_test = pd.read_csv(\"data/Higgs_test.csv\")\n",
    "\n",
    "print(\n",
    "    f\"Our data contains {len(data_train):,} training samples \"\n",
    "    f\"and {len(data_test):,} test samples.\\n\"\n",
    ")\n",
    "\n",
    "print(\"TRAINING DATA INFORMATION:\\n\")\n",
    "data_train.info()\n",
    "\n",
    "print(\"\\nTRAINING DATA HEAD:\")\n",
    "display(data_train.head())\n",
    "\n",
    "print(\"\\nTRAINING DATA SUMMARY STATISTICS:\")\n",
    "display(data_train.describe())\n",
    "\n",
    "# Split dataframe into X and y numpy arrays\n",
    "X_train = data_train.iloc[:, data_train.columns != \"class\"].values\n",
    "y_train = data_train[\"class\"].values\n",
    "X_test = data_test.iloc[:, data_test.columns != \"class\"].values\n",
    "y_test = data_test[\"class\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part1\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 1: Decision Tree [21 pts]</div> \n",
    "    \n",
    "[Return to contents](#contents)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1** In this problem, we will observe how both tree-depth and cross-validation affect our ability to accurately model data. Specifically, for each tree depth from 1 to 20 (inclusive):\n",
    "\n",
    "- Fit a decision tree to the entire **training** set.\n",
    "\n",
    "- Evaluate on the entire **training** set (i.e., `.score(...)`), while storing the scores in a variable named `train_scores`.\n",
    "\n",
    "- Perform 5-fold cross-validation with the entire **training** set using [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html). Store the mean validation score and the validation standard deviation  in variables named `cvmeans` and `cvstds` respectively.\n",
    "</div>\n",
    "\n",
    "_Points:_ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "    \n",
    "**1.2** Now that we have `train_scores`, `cvmeans`, and `cvstds`, let's plot them. Generate 2 plots, both showing (a) the non-cross-validation training scores, (b) the mean validation scores, and (c) a shaded region that illustrates the +/-2 standard deviation validation bounds for each tree depth. The content and formatting of these 2 plots should be identical, EXCEPT in one plot set the limits on the y-axis to focus on the validation performance. Remember to label and title each plot appropropriately.\n",
    "\n",
    "**HINT:** You can use `plt.fill_between(...)` to easily generate the shaded region in your plots.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3** Using the cross-validation experiments from above, select a depth you deem most appropriate for using on future, unseen data, store it in `best_cv_depth`, and **justify your choice**. Then, using this depth, fit a new decision tree on the entire training data and store the train and test accuracies in `best_cv_tree_train_score` and `best_cv_tree_test_score`, respectively, which we will refer to in later questions.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# choose best depth after a qualitative assessment of our plots\n",
    "best_cv_depth = ...\n",
    "best_cv_tree_train_score = ...\n",
    "best_cv_tree_test_score = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# print model results summary\n",
    "print(\n",
    "    \"The tree of max-depth {} trained on the \"\n",
    "    \"full training set, achieves the following accuracy scores:\"\n",
    "    \"\\n\\n\\ttrain\\t{:.4f}\\n\\tTEST\\t{:.4f}\".format(\n",
    "        best_cv_depth,\n",
    "        best_cv_tree_train_score,\n",
    "        best_cv_tree_test_score,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4** In terms of the bias-variance tradeoff, how does limiting tree depth avoid over-fitting? What is one downside of limiting the tree depth?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part2\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 2: Bagging [20 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**2.1** Based on your results from [Question 1](#part1solutions), choose a tree depth that WILL overfit the training set. What evidence leads you to believe that this depth overfits? Assign your choice to a variable named `tree_depth`. (You may want to explore different settings for this value in the problems below. However, be certain that your final choice and rationale is based on your results from Question 1.)\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set overfitting tree_depth based on provided rationale \n",
    "tree_depth = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2** Here we will use the `tree_depth` chosen in Question 2.1 to generate 55 boostrapped sets of decision tree predictions for both the training and test data. To accomplish this:\n",
    "\n",
    "- Using a random seed of 0, call the provided `bagger` function to return your bootstrapped results.\n",
    "- Store your returned results as: \n",
    "  1. `bagging_train_df`: a dataframe containing your training data predictions (see the \"required dataframe structure\" below)\n",
    "  2. `bagging_test_df`: a dataframe containing your test data predictions\n",
    "  3. `bagging_models_list`: a list containing your 55 fitted model objects (i.e. fitted estimators)\n",
    "- Finally, display the heads of both dataframes.\n",
    "\n",
    "**NOTE:** There is no need to do anything with your `bagging_models_list` list yet. It will not be used until later in [Question 3.2](#part3).\n",
    "\n",
    "**DATAFRAME STRUCTURE:** The training and test prediction results of your bootstraps are returned by the `bagger` function as dataframes formatted like the example shown below. Each row represents one observation (from either the training or test set depending on the dataframe), and each column represents one bootstrapped result. The values stored in the dataframe are the bootstrapped predictions for each observation as illustrated below.\n",
    "\n",
    "An example of the `bagging_train_df` and `bagging_test_df` dataframes would be:\n",
    "\n",
    "|     |bootstrap model 1|bootstrap model 2|...|bootstrap model 55|  \n",
    "| --- | --- | --- | --- | --- |\n",
    "|0| 0 | 1|... |0|\n",
    "|1| 1| 1|... |0|\n",
    "|2| 0| 0|... |1|\n",
    "|...| ...| ...|... |... |\n",
    "| $n$-1 | 0| 0|... |1|\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def bagger(\n",
    "    n_trees: int,\n",
    "    tree_depth: int,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    random_seed: int = 0,\n",
    ") -> (pd.DataFrame, pd.DataFrame, list):\n",
    "    \"\"\"Generate boostrapped DecisionTreeClassifier predictions\n",
    "    \n",
    "    Function fits bootstrapped DecisionTreeClassifier models\n",
    "    and returns training and test predictions for each of those\n",
    "    bootstrapped models, along with the fitted model objects as\n",
    "    described in Question 2.2 question text.\n",
    "    \n",
    "    :param n_trees: int, number of bootstrapped decision trees\n",
    "    :param tree_depth: int, maximum tree depth\n",
    "    :param X_train: np.ndarray, training X observations\n",
    "    :param y_train: np.ndarray, training y observations\n",
    "    :param X_test: np.ndarray, test X observations\n",
    "    :param random_seed: int, random seed used to set np.random.seed\n",
    "                        to ensure replicable results (default=0)\n",
    "    \n",
    "    :returns: (pd.DataFrame, pd.DataFrame, list), tuple containing 3\n",
    "              objects, (1) bagging_train_df dataframe\n",
    "              as described in Q2.2 question text, (2) bagging_test_df\n",
    "              dataframe as described in Q2.2, and (3) bagging_models_list\n",
    "              containing every trained DecisionTreeClassifier model\n",
    "              object (i.e. estimator), one estimator for each bootstrap\n",
    "              (you will need this list later in Q3.2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # set random seed for replicable results\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # instantiate arrays and list for storing results\n",
    "    bagging_train = np.zeros((X_train.shape[0], n_trees)).astype(int)\n",
    "    bagging_test = np.zeros((X_test.shape[0], n_trees)).astype(int)\n",
    "\n",
    "    bagging_models_list = []\n",
    "\n",
    "    # perform n bootstraps\n",
    "    for i in range(n_trees):\n",
    "        # generate bootstrapped model\n",
    "        bootstrapped_X, bootstrapped_y = resample(X_train, y_train)\n",
    "        fitted_model = DecisionTreeClassifier(\n",
    "            max_depth=tree_depth\n",
    "        ).fit(bootstrapped_X, bootstrapped_y)\n",
    "        bagging_models_list.append(fitted_model)\n",
    "\n",
    "        # predict on full training and test sets and store\n",
    "        # results to arrays\n",
    "        bagging_train[:,i] = fitted_model.predict(X_train)\n",
    "        bagging_test[:,i] = fitted_model.predict(X_test)\n",
    "    \n",
    "    # convert arrays to pandas dataframes as required\n",
    "    bagging_train_df = pd.DataFrame(\n",
    "        bagging_train[:, :],\n",
    "        columns=[f\"model{x}\" for x in range(n_trees)],\n",
    "    )\n",
    "    bagging_test_df = pd.DataFrame(\n",
    "        bagging_test[:, :],\n",
    "        columns=[f\"model{x}\" for x in range(n_trees)],\n",
    "    )\n",
    "    \n",
    "    return bagging_train_df, bagging_test_df, bagging_models_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify number of required bootrapped trees\n",
    "n_trees = 55 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate predictions using bagger function\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# display resulting dataframe heads\n",
    "display(bagging_train_df.head())\n",
    "display(bagging_test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3**  Aggregate all 55 bootstrapped models to get a combined prediction for each training and test observation (i.e. predict a `1` if, and only if, a majority of the models predict that observation to be from class 1). Assign the bagging train and test accuracies to variables named `bagging_accuracy_train` and `bagging_accuracy_test`.\n",
    "\n",
    "**HINT:** You can use `np.mean(...)` to easily test for majority. If a majority of models vote 1, consider what that implies about the mean.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate accuracy on our bagged prediction results\n",
    "bagging_accuracy_train = ...\n",
    "bagging_accuracy_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# print summary of results\n",
    "print(\n",
    "    f\"Bagging {n_trees} trees of depth-{tree_depth} achieves \"\n",
    "    f\"the following accuracy scores:\\n\\n\\ttrain\\t\"\n",
    "    f\"{bagging_accuracy_train:.4f}\\n\\tTEST\\t\"\n",
    "    f\"{bagging_accuracy_test:.4f}\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"Our prior single depth-{best_cv_depth} tree achieved a \"\n",
    "    f\"TEST score of {best_cv_tree_test_score:.4f}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.4",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**2.4** We want to know how the number of bootstraps affects our bagging ensemble's performance. Use the `running_predictions(...)` function provided below to get the model's accuracy score when using only $j$ of the bootstrapped models, where $j \\in [1, 2, 3, ..., 55]$. Using the `tree_depth` chosen in Question 2.1, make a plot that illustrates the accuracy on the training set and test set at each number of bootstraps (varying $j$ from 1 to 55). Please see the `running_predictions` signature and docstring regarding the use of the function. You should be able to use your `bagger`-generated dataframes from Q2.2 as an input to this function.\n",
    "\n",
    "On your plot, in addition to the training and test accuracies at each value $j$, also include horizontal lines for two baseline comparisons:\n",
    "\n",
    "1. The test accuracy of the best model from [Question 1](#part1solutions);\n",
    "2. The test accuracy of a single decision tree with the overfit `tree_depth` you chose in Question 2.1, trained on the full training set.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "# DO NOT MODIFY THIS CODE\n",
    "\n",
    "def running_predictions(\n",
    "    bagger_results_df: pd.DataFrame,\n",
    "    targets: np.ndarray,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Generates running accuracy of intermediate bootstraps when bagging \n",
    "    \n",
    "    Generates a series of accuracy scores calculated using the\n",
    "    running predictions of each additional bootstrapped model\n",
    "    generated using the Question 2.2 `bagger` function. For example,\n",
    "    the first accuracy in the series represents the predictive\n",
    "    accuracy of just the first bootstrapped model. The second accuracy\n",
    "    reflects the bagged accuracy of the first 2 bootstrapped models. The\n",
    "    j-th accuracy reflects the bagged accuracy of the first j\n",
    "    bootstrapped models.\n",
    "    \n",
    "    :param bagger_results_df: pd.DataFrame, a bagging results dataframe\n",
    "                              (either train or test) output from the Q2.2\n",
    "                              `bagger` function\n",
    "    :param targets: np.ndarray, 1-dimensional array of true class labels\n",
    "                    for either train or test observations (i.e y_train or\n",
    "                    y_test, whichever corresponds to the inputted\n",
    "                    bagger_results_df)             \n",
    "    :returns: pd.Series, a series of values showing the accuracy of\n",
    "              using the initial j trees to predict the targets for each\n",
    "              value of j bootstrapped models\n",
    "    \"\"\"\n",
    "    # verify that input data objects meet the requirements specified\n",
    "    # in the docstring\n",
    "    assert type(bagger_results_df)==pd.core.frame.DataFrame, (\n",
    "        \"bagger_results_df input must be a pd.DataFrame\"\n",
    "    )\n",
    "    assert type(targets)==np.ndarray, (\n",
    "        \"targets input must be an np.ndarray\"\n",
    "    )\n",
    "    assert targets.ndim==1, (\n",
    "        \"targets input np.ndarray must be one-dimensional\"\n",
    "    )\n",
    "    \n",
    "    # identify the number of bootstrapped trees in inputted bagger df\n",
    "    n_trees = bagger_results_df.shape[1]\n",
    "    \n",
    "    # calculate the running percentage of models voting 1 as each\n",
    "    # additional model is considered\n",
    "    running_percent_1s = (\n",
    "        np.cumsum(bagger_results_df, axis=1)/np.arange(1,n_trees+1)\n",
    "    )\n",
    "    \n",
    "    # predict 1 when the running average is above 0.5\n",
    "    running_conclusions = running_percent_1s > 0.5\n",
    "    \n",
    "    # check whether the running predictions match the targets\n",
    "    running_correctnesss = running_conclusions == targets.reshape(-1,1)\n",
    "    \n",
    "    # calculate and return final accuracies\n",
    "    return np.mean(running_correctnesss, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.5",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**2.5** Referring to your graph from 2.4, compare the performance of bagging against the baseline of a single `tree_depth` tree. Explain what you see in terms of the differences between how bagging and limiting tree depth work to control overfitting.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part3\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 3: Random Forests [14 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "3.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.1**  Fit a `RandomForestClassifier` to the original `X_train` data using the same tree depth and number of trees you used in Question 2.2, and set the maximum number of features to use when looking for the best split to be the square root of the total number of features. Evaluate classifier's accuracy on the training and test sets and store them in `random_forest_train_score` and `random_forest_test_score`.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate results\n",
    "random_forest_train_score = ...\n",
    "random_forest_test_score = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print results summary\n",
    "print(\n",
    "    \"The random forest of depth-{} and {} trees achieves the \"\n",
    "    \"following accuracy scores:\\n\\n\\ttrain\\t{:.4f}\\n\\tTEST\\t{:.4f}\"\n",
    "    .format(\n",
    "        tree_depth,\n",
    "        n_trees,\n",
    "        random_forest_train_score,\n",
    "        random_forest_test_score,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "3.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**3.2** Among all of the decision trees you fit in the bagging process (i.e. each of the fitted model objects stored in `bagging_models_list`), how many times is each feature used as the top/first node? How about for each tree in the random forest you just fit? Assign the answers to these questions to two pandas Series Dataframes called `top_predictors_bagging` and `top_predictors_rf`, and display them.\n",
    "\n",
    "What about the process of training the Random Forest causes this difference? What implication does this observation have on the accuracy of bagging vs. random forest?\n",
    "\n",
    "**HINT:** A decision tree's top feature is stored as `.tree_.feature[0]`. A random forest object stores its decision trees in its `.estimators_` attribute.\n",
    "    \n",
    "**IMPORTANT:** As always, your output should be easy to interpret. In this context that means construcing your DataFrames with the care, assigning appropriate column names and/or index values to best convey to the reader what the output represents.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "3.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**3.3**: Make a Pandas DataFrame (following the expected structure shown below) of the training and test accuracy for the following models and name it `results_df`:\n",
    "\n",
    "1. Single tree with the best depth chosen by cross-validation (from Question 1)\n",
    "2. A single overfit tree trained on all data (from Question 2, using the depth you chose there)\n",
    "3. Bagging 55 such trees (from Question 2)\n",
    "4. A random forest of 55 such trees (from Question 3.1)\n",
    "\n",
    "Display your `results_df` dataframe and answer: What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance tradeoff.\n",
    "\n",
    "**NOTE:** This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.\n",
    "\n",
    "The expected structure for `results_df` is:  \n",
    "\n",
    "| classifier | training accuracy | test accuracy |\n",
    "| --- |  --- | --- |\n",
    "| single depth-$i$ tree chosen by CV | ... | ... |\n",
    "| single overfit depth-$k$ tree | ... | ... |\n",
    "| bagging 55 depth-$k$ trees | ... | ... |\n",
    "| random forest of 55 depth-$k$ trees | ... | ... |\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# display results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part4\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 4: Boosting [30 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.1** The following code (see code cell below) \"attempts\" to implement a simplified version of boosting using just two classifiers. However, this implementation has both fuctionality AND stylistic flaws. Imagine that you are a grader for a college course in Data Science. Write a set of grading comments (in the provided Markdown cell) for the student who submitted this code. Point out the flaws in their provided code submission.\n",
    "\n",
    "The intended functionality (i.e. expected requirements) of this \"attempted\" code is to accomplish the following:\n",
    "\n",
    "1. Fit an initial tree with a maximum depth of 3.\n",
    "2. Construct an array of sample weights that give a weight of 1 to samples that the initial tree classified correctly, and a weight of 2 to samples that the initial tree misclassified.\n",
    "3. Fit a second depth-3 decision tree using those sample weights.\n",
    "4. Predict by computing the probabilities that the initial tree and the second tree each assign to the positive class, then take the average of those two probabilities as the prediction probability.\n",
    "5. Report the training and test accuracies of just the initial tree, as well the training and test accuracies of the full 2-tree boosting ensemble.\n",
    "\n",
    "**NOTE:** Please do not modify anything in the code cell itself.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "# \"Attempted\" boosting implementation\n",
    "\n",
    "def boostmeup(X, y):\n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "    tree1 = tree.fit(X, y)\n",
    "    sample_weight = np.ones(len(X_train))\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "          if tree1.predict([X_train[idx]]) != y_train[idx]:\n",
    "             sample_weight[idx] = sample_weight[idx] * 2\n",
    "             q = q + 1\n",
    "    print(\"tree1 accuracy:\", q / len(X_train))\n",
    "    tree2 = tree.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    \n",
    "# Train\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "        t1p = tree1.predict_proba([X_train[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_train[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_train))\n",
    "\n",
    "# Test\n",
    "    q = 0\n",
    "    for idx in range(len(X_test)):\n",
    "        t1p = tree1.predict_proba([X_test[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_test[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_test))\n",
    "\n",
    "boostmeup(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.2** Now, imagine that you are the Teaching Fellow responsible for writing the \"solutions\" code for the simplified version of boosting using just two classifiers that had been \"attempted\" in Question 4.1:\n",
    "\n",
    "- Write an **excellent** example implementation from scratch (i.e. using just scikit-learn's `DecisionTreeClassifier` and NumPy to perform your boosting). Your implementation should be written either [functionally](https://docs.python.org/3/tutorial/controlflow.html#defining-functions) or as a [class](https://docs.python.org/3/tutorial/classes.html), such that you can then call the function(s) or class methods to generate your predictions and/or accuracy scores.\n",
    "\n",
    "- Report on the performance of your boosting algorithm by printing the training and test accuracies of just the initial tree, as well the training and test accuracies of the full 2-tree boosting ensemble.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.3** Now, let us use the scikit-learn implementation of AdaBoost. For the sake of simplicity we'll use a single validation split for hyperparameter tuning.\n",
    "    \n",
    "Use `AdaBoostClassifier` to fit another ensemble to the reduced training set, `X_train80`. Use a decision tree of depth-3 as the base learner, a learning rate 0.05, the default algorithm `SAMME.R`, and run the boosting for 800 iterations. Make a plot of the effect of the number of iterations on the model's train and validation accuracy.\n",
    "\n",
    "**HINT:** The `.staged_score(...)` method provides the accuracy numbers you'll need for plotting. You'll need to use `list(...)` to convert the \"generator\" that `staged_score` returns into an ordinary list.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train/validation split for hyperparameter tuning\n",
    "X_train80, X_val, y_train80, y_val = train_test_split(X_train,\n",
    "                                                      y_train,\n",
    "                                                      train_size=.8,\n",
    "                                                      random_state=109,\n",
    "                                                      stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.4",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.4** Repeat the plot above for a base learner with depths of 1, 2, 3, and 4. For easier comparison you should arrange your plots in a row. What trends do you see in the training and validation accuracies and how would you explain this behavior?\n",
    "\n",
    "**NOTE:** It is okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.5** Based on the plots from Question 4.4, what combination of base learner depth and the number of iterations seems optimal and why?\n",
    "\n",
    "**Note:** Feel free to make use of additional code to make your decision here if that is helpful. \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.6** Fit a final AdaBoostClassifier on the **entire train set** using the base learner depth and number of iterations you identified as optimal in the previous question. Keep the learning rate at 0.05 as before. Store the train and test accuracies in `ada_train_acc` and `ada_test_acc` respectively. How does the performance of this model compare with the performance of the ensemble methods you considered in Question 2 and Question 3?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "ada_train_acc = ...\n",
    "ada_test_acc = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Ada Boost Train Accuracy: {ada_train_acc:.2%}\")\n",
    "print(f\"Ada Boost Test Accuracy: {ada_test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part5\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 5: Understanding [15 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "This question is intended to evaluate your overall knowledge and understanding of the current material. You may need to refer to lecture notes and other course materials to answer these questions.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**5.1** How do boosting and bagging relate: what is common to both, and what is unique to each?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**5.2** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.4",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**5.3** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time? Why?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.5",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**5.4** Which of these techniques can be extended to regression tasks? Describe how this can be done.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 7,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(train_scores) == 20, \"Your train_scores should be a list or array containing 20 values\"\n>>> assert len(cvmeans) == 20, \"Your cvmeans should be a list or array containing 20 values\"\n>>> assert len(cvstds) == 20, \"Your cvstds should be a list or array containing 20 values\"\n>>> assert np.isclose(np.mean(train_scores), 0.84211, atol=0.001), \"The values in your train_scores list/array are incorrect\"\n>>> assert np.isclose(np.mean(cvmeans), 0.6103, atol=0.001), \"The values in your cvmeans list/array are incorrect\"\n>>> assert np.isclose(np.mean(cvstds), 0.0103326, atol=0.001), \"The values in your cvstds list/array are incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert bagging_train_df.shape == (5000, 55), \"Check the dimensions of your bagging_train_df\"\n>>> assert bagging_test_df.shape == (5000, 55), \"Check the dimensions of your bagging_test_df\"\n>>> assert len(bagging_models_list) == 55, \"You should have 55 estimators in your bagging_models_list\"\n>>> assert bagging_train_df.sum().sum() == 144604, \"The values in your bagging_train_df are incorrect. Make sure you are using the right random seed.\"\n>>> assert bagging_test_df.sum().sum() == 145385, \"The values in your bagging_test_df are incorrect. Make sure you are using the right random seed.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.3": {
     "name": "q2.3",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert bagging_accuracy_train > 0.9 and bagging_accuracy_train <= 1, \"Your bagging train accuracy is very off\"\n>>> assert bagging_accuracy_test > 0.60 and bagging_accuracy_test < 0.8, \"Your bagging test accuracy is very off\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert random_forest_train_score > 0.9 and random_forest_train_score <= 1, \"Your RF train accuracy is very off\"\n>>> assert random_forest_test_score > 0.60 and random_forest_test_score < 0.8, \"Your RF test accuracy is very off\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.2": {
     "name": "q3.2",
     "points": 6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert type(top_predictors_bagging) == type(pd.DataFrame()), \"top_predictors_bagging should be a DataFrame\"\n>>> assert type(top_predictors_rf) == type(pd.DataFrame()), \"top_predictors_rf should be a DataFrame\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
